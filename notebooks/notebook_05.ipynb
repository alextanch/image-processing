{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7233aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mediapy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e769f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт зависимостей\n",
    "import cv2\n",
    "import mediapy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# настройка matplotlib\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")\n",
    "\n",
    "# путь к данным\n",
    "DATA_DIR = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b9df8",
   "metadata": {},
   "source": [
    "#### **Детектор ключевых точек SIFT**\n",
    "\n",
    "##### **Читаем изображения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = cv2.imread(DATA_DIR / \"brandenburg_gate_1.jpg\", cv2.IMREAD_COLOR_RGB)\n",
    "image2 = cv2.imread(DATA_DIR / \"brandenburg_gate_2.jpg\", cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "mediapy.show_images({\"image1\": image1, \"image2\": image2}, border=True, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0edd5bd",
   "metadata": {},
   "source": [
    "##### **Создание SIFT детектора**\n",
    "\n",
    "- `nfeature`: максимальное число лучших особых точек\n",
    "\n",
    "- `nOctaveLayers`: число слоев в каждой октаве\n",
    "\n",
    "- `contrastThreshold`: порог для фильтрации слабых особенностей по контрасту\n",
    "\n",
    "- `edgeThreshold`: порог для фильтрации граничных точек\n",
    "\n",
    "- `sigma`: $\\sigma_1$ в первой октаве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f7340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create(\n",
    "    nfeatures = 1000,\n",
    "    edgeThreshold = 6,\n",
    "    contrastThreshold = 0.04,\n",
    "    nOctaveLayers = 3,\n",
    "    sigma = 1.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffc4714",
   "metadata": {},
   "source": [
    "##### **Вычислим и отобразим особые точки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f414fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints1, descriptors1 = sift.detectAndCompute(image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(image2, None)\n",
    "\n",
    "output_image1 = image1.copy()\n",
    "cv2.drawKeypoints(image1, keypoints1, output_image1);\n",
    "\n",
    "output_image2 = image2.copy()\n",
    "cv2.drawKeypoints(image2, keypoints2, output_image2);\n",
    "\n",
    "mediapy.show_images({\n",
    "    \"output_image1\": output_image1, \n",
    "    \"output_image2\": output_image2}, \n",
    "    border=True, width=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e568e",
   "metadata": {},
   "source": [
    "##### **У каждой особой точки есть атрибуты**\n",
    "- координаты\n",
    "- ориентация\n",
    "- размер\n",
    "- дескриптор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b73895",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "kp = keypoints1[n]\n",
    "desc = descriptors1[n]\n",
    "\n",
    "print(f\"Coordinates of keypoint {n}: {kp.pt}\")\n",
    "print(f\"Angle of keypoint {n}:       {kp.angle:.2f}\")\n",
    "print(f\"Size of keypoint {n}:        {kp.size:.2f}\")\n",
    "print(f\"Descriptor of keypoint {n}:\\n{np.array_str(desc, precision=6, suppress_small=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee7b60",
   "metadata": {},
   "source": [
    "##### **Отобразим ориентацию и размер особых точек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18726d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawKeypoints(\n",
    "    image1, \n",
    "    keypoints1, \n",
    "    output_image1, \n",
    "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "\n",
    "mediapy.show_image(output_image1, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23fc446",
   "metadata": {},
   "source": [
    "##### **Brute-Force Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3224d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2)\n",
    "\n",
    "matches = bf.knnMatch(descriptors1, descriptors2, k=1)\n",
    "\n",
    "# Draw matches.\n",
    "matches_image = cv2.drawMatchesKnn(image1,\n",
    "    keypoints1,\n",
    "    image2,\n",
    "    keypoints2,\n",
    "    matches,\n",
    "    outImg=None,\n",
    ")\n",
    "\n",
    "mediapy.show_image(matches_image, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c1695",
   "metadata": {},
   "source": [
    "##### **Brute-Force Cross Check Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fea437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    " \n",
    "# Match descriptors.\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    " \n",
    "# Sort them in the order of their distance.\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    " \n",
    "# Draw first 200 matches.\n",
    "matches_image = cv2.drawMatches(\n",
    "    image1, keypoints1,\n",
    "    image2, keypoints2,\n",
    "    matches[:200],\n",
    "    None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "mediapy.show_image(matches_image, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afc4ab",
   "metadata": {},
   "source": [
    "##### **FLANN Matching with Ratio Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN matcher\n",
    "flann = cv2.FlannBasedMatcher(\n",
    "    {\"algorithm\": 1, \"trees\": 5}\n",
    ")\n",
    " \n",
    "matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    " \n",
    "# Apply ratio test\n",
    "good = []\n",
    "\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.6 * n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "matches_image = cv2.drawMatchesKnn(\n",
    "    image1, keypoints1,\n",
    "    image2, keypoints2,\n",
    "    good,\n",
    "    None,\n",
    "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "mediapy.show_image(matches_image, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436ebf2",
   "metadata": {},
   "source": [
    "#### **Детектор ключевых точек SuperPoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ac991",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperPointNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        c1, c2, c3, c4, c5, d1 = 64, 64, 128, 128, 256, 256\n",
    "        \n",
    "        # Shared Encoder.\n",
    "        self.conv1a = nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1b = nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2a = nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2b = nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3a = nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3b = nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4a = nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4b = nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Detector Head.\n",
    "        self.convPa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "        self.convPb = nn.Conv2d(c5, 65, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # Descriptor Head.\n",
    "        self.convDa = nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "        self.convDb = nn.Conv2d(c5, d1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, image):\n",
    "        x = torch.from_numpy(cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)) / 255.0\n",
    "        x = x[None, None]\n",
    "\n",
    "        # Shared Encoder.\n",
    "        x = self.relu(self.conv1a(x))\n",
    "        x = self.relu(self.conv1b(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2a(x))\n",
    "        x = self.relu(self.conv2b(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3a(x))\n",
    "        x = self.relu(self.conv3b(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv4a(x))\n",
    "        x = self.relu(self.conv4b(x))\n",
    "\n",
    "        # Detector Head.\n",
    "        cPa = self.relu(self.convPa(x))\n",
    "        semi = self.convPb(cPa)\n",
    "\n",
    "        # Descriptor Head.\n",
    "        cDa = self.relu(self.convDa(x))\n",
    "        desc = self.convDb(cDa)\n",
    "        dn = torch.norm(desc, p=2, dim=1)  # Compute the norm.\n",
    "        coarse_desc = desc.div(torch.unsqueeze(dn, 1))  # Divide by norm to normalize.\n",
    "\n",
    "        # Postprocessing\n",
    "        semi = semi.cpu().numpy().squeeze()\n",
    "        dense = np.exp(semi) # Softmax.\n",
    "        dense = dense / (np.sum(dense, axis=0) + .00001) # Should sum to 1.\n",
    "        # Remove dustbin.\n",
    "        nodust = dense[:-1, :, :]\n",
    "\n",
    "        h, w = nodust.shape[1:3]\n",
    "        H, W = 8 * h, 8 * w\n",
    "\n",
    "        nodust = nodust.transpose(1, 2, 0)\n",
    "        heatmap = np.reshape(nodust, [h, w, 8, 8])\n",
    "        heatmap = np.transpose(heatmap, [0, 2, 1, 3])\n",
    "        heatmap = np.reshape(heatmap, [h * 8, w * 8])\n",
    "        xs, ys = np.where(heatmap >= 0.015) # Confidence threshold.\n",
    "\n",
    "        pts = np.zeros((3, len(xs))) # Populate point data sized 3xN.\n",
    "        pts[0, :] = ys\n",
    "        pts[1, :] = xs\n",
    "        pts[2, :] = heatmap[xs, ys]\n",
    "\n",
    "        pts, _ = self.nms_fast(pts, H, W, dist_thresh=4) # Apply NMS.\n",
    "        inds = np.argsort(pts[2,:])\n",
    "        pts = pts[:, inds[::-1]] # Sort by confidence.\n",
    "\n",
    "        # Interpolate into descriptor map using 2D point locations.\n",
    "        samp_pts = torch.from_numpy(pts[:2, :].copy())\n",
    "        samp_pts[0, :] = (samp_pts[0, :] / (float(W)/2.)) - 1.\n",
    "        samp_pts[1, :] = (samp_pts[1, :] / (float(H)/2.)) - 1.\n",
    "        samp_pts = samp_pts.transpose(0, 1).contiguous()\n",
    "        samp_pts = samp_pts.view(1, 1, -1, 2)\n",
    "        samp_pts = samp_pts.float()\n",
    "        desc = nn.functional.grid_sample(coarse_desc, samp_pts, align_corners=True)\n",
    "        D = coarse_desc.shape[1]\n",
    "        desc = desc.cpu().numpy().reshape(D, -1)\n",
    "        desc /= np.linalg.norm(desc, axis=0)[np.newaxis, :]\n",
    "\n",
    "        keypoints = pts[:2].T.astype(int)\n",
    "        descriptors = desc.T.astype(np.float32)\n",
    "\n",
    "        return keypoints, descriptors\n",
    "\n",
    "    def nms_fast(self, in_corners, H, W, dist_thresh):\n",
    "        grid = np.zeros((H, W)).astype(int) \n",
    "        inds = np.zeros((H, W)).astype(int) \n",
    "        \n",
    "        # Sort by confidence and round to nearest int.\n",
    "        inds1 = np.argsort(-in_corners[2, :])\n",
    "        corners = in_corners[:, inds1]\n",
    "        rcorners = corners[:2, :].round().astype(int)  # Rounded corners.\n",
    "        \n",
    "        # Check for edge case of 0 or 1 corners.\n",
    "        if rcorners.shape[1] == 0:\n",
    "            return np.zeros((3, 0)).astype(int), np.zeros(0).astype(int)\n",
    "        \n",
    "        if rcorners.shape[1] == 1:\n",
    "            out = np.vstack((rcorners, in_corners[2])).reshape(3, 1)\n",
    "            return out, np.zeros((1)).astype(int)\n",
    "        \n",
    "        # Initialize the grid.\n",
    "        for i, rc in enumerate(rcorners.T):\n",
    "            grid[rcorners[1, i], rcorners[0, i]] = 1\n",
    "            inds[rcorners[1, i], rcorners[0, i]] = i\n",
    "        \n",
    "        # Pad the border of the grid, so that we can NMS points near the border.\n",
    "        pad = dist_thresh\n",
    "        grid = np.pad(grid, ((pad, pad), (pad, pad)), mode=\"constant\")\n",
    "        \n",
    "        # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
    "        count = 0\n",
    "        \n",
    "        for i, rc in enumerate(rcorners.T):\n",
    "            # Account for top and left padding.\n",
    "            pt = (rc[0] + pad, rc[1] + pad)\n",
    "        \n",
    "            if grid[pt[1], pt[0]] == 1:  # If not yet suppressed.\n",
    "                grid[pt[1] - pad : pt[1] + pad + 1, pt[0] - pad : pt[0] + pad + 1] = 0\n",
    "                grid[pt[1], pt[0]] = -1\n",
    "                count += 1\n",
    "        \n",
    "        # Get all surviving -1's and return sorted array of remaining corners.\n",
    "        keepy, keepx = np.where(grid == -1)\n",
    "        keepy, keepx = keepy - pad, keepx - pad\n",
    "        inds_keep = inds[keepy, keepx]\n",
    "        out = corners[:, inds_keep]\n",
    "        values = out[-1, :]\n",
    "        inds2 = np.argsort(-values)\n",
    "        out = out[:, inds2]\n",
    "        out_inds = inds1[inds_keep[inds2]]\n",
    "        \n",
    "        return out, out_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e147a",
   "metadata": {},
   "source": [
    "##### **Создание и загрузка весов обученной модели SuperPointNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69245677",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SuperPointNet()\n",
    "net.load_state_dict(torch.load(DATA_DIR / \"superpoint.pth\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cbf291",
   "metadata": {},
   "source": [
    "##### **Network inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a69584",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts1, des1 = net(image1)\n",
    "pts2, des2 = net(image2)\n",
    "\n",
    "print(f\"pts1 shape: {pts1.shape}\")\n",
    "print(f\"pts2 shape: {pts2.shape}\\n\")\n",
    "\n",
    "print(f\"des1 shape: {des1.shape}\")\n",
    "print(f\"des2 shape: {des2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eed065",
   "metadata": {},
   "source": [
    "##### **Отобразим ключевые точки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcef462",
   "metadata": {},
   "outputs": [],
   "source": [
    "otput_image = np.concatenate((image1, image2), axis=1)\n",
    "\n",
    "H, W = image2.shape[:2]\n",
    "\n",
    "for x, y in pts1:\n",
    "    cv2.circle(otput_image, (x, y), 3, (255, 0, 255), -1)\n",
    "\n",
    "for x, y in pts2:\n",
    "    cv2.circle(otput_image, (x + W, y), 3, (255, 0, 255), -1)\n",
    "\n",
    "mediapy.show_image(otput_image, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc1689",
   "metadata": {},
   "source": [
    "##### **Вычисление матрицы квадратов расстояний между дескрипторами**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = np.dot(des2, des1.T)\n",
    "D = 2 * (1 - np.clip(dots, -1, 1))\n",
    "\n",
    "print(f\"D shape: {D.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3176ac",
   "metadata": {},
   "source": [
    "#### **Домашняя работа 5**\n",
    "\n",
    "##### **Теория**\n",
    "\n",
    "- Feature Matching (постановка задачи и общий подход к решению) \n",
    "\n",
    "- вычисление ключевых точек в алгоритме SIFT\n",
    "\n",
    "- вычисление SIFT дескрипторов\n",
    "\n",
    "- Сопоставление ключевых точек\n",
    "    - Ratio Test Matching\n",
    "    - Cross-Check Matching\n",
    "\n",
    "##### **Задача**\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../data/sp_matches.jpg\" width=\"600\"/>\n",
    "</figure>\n",
    "\n",
    "- для двух изображений, вычислите матрицу квадратов расстояний SuperPoint дескрипторов\n",
    "\n",
    "- используя матрицу квадратов расстояний и не используя циклы,\n",
    "реализуйте Cross-Check Matching алгоритм\n",
    "\n",
    "- отобразите пары соответсвующих точек на выходном изображении\n",
    "\n",
    "- параметры в файле `params.yaml`:\n",
    "\n",
    "```yaml\n",
    "    homework_05:\n",
    "        # Input images\n",
    "        input_image_1: data/brandenburg_gate_1.jpg\n",
    "        input_image_2: data/brandenburg_gate_2.jpg\n",
    "        # Output image\n",
    "        output_image: data/sp_matches.jpg\n",
    "        # distance threshold\n",
    "        T: 0.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91236b48",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
