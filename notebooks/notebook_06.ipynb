{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4402ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mediapy torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт зависимостей\n",
    "import cv2\n",
    "import mediapy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# настройка matplotlib\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")\n",
    "\n",
    "# путь к данным\n",
    "DATA_DIR = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e54bcc",
   "metadata": {},
   "source": [
    "#### **Пороговая сегментация изображения**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e5939",
   "metadata": {},
   "source": [
    "##### **Построим гистограмму изображения**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем grayscale изображение\n",
    "src_image = cv2.imread(DATA_DIR / \"gaussian_noise.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# вычисление гистограммы\n",
    "hist = cv2.calcHist([src_image], [0], None, [256], [0, 256])\n",
    "\n",
    "# графики\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(src_image, cmap=\"gray\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist)\n",
    "plt.xlim([0, 256])\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cfb1c",
   "metadata": {},
   "source": [
    "##### **Cгладим изображение и выберем порог сегментации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f779e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# гауссовское сглаживание 5 x 5\n",
    "blur_image = cv2.GaussianBlur(src_image, (5, 5), sigmaX=0)\n",
    "\n",
    "# вычисление гистограммы\n",
    "hist = cv2.calcHist([blur_image], [0], None, [256], [0, 256])\n",
    "\n",
    "# порог сегментации\n",
    "T = 130\n",
    "\n",
    "# графики\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(blur_image, cmap=\"gray\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(hist)\n",
    "plt.axvline(x=T, color=\"r\", linestyle=\"--\", label=f\"T = {T}\")\n",
    "plt.xlim([0, 256])\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55134ac2",
   "metadata": {},
   "source": [
    "##### **Выполним пороговую сегментацию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c326c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пороговая сегментация\n",
    "T = 130\n",
    "mask = np.uint8(255 * (blur_image > T))\n",
    "\n",
    "# графики\n",
    "mediapy.show_images({\n",
    "    \"исходное изображение\": src_image, \n",
    "    f\"сегментация по порогу T = {T}\": mask\n",
    "}, border=True, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95211107",
   "metadata": {},
   "source": [
    "##### **Сегментация с помощью порога Оцу**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем grayscale изображение\n",
    "image = cv2.imread(DATA_DIR / \"polymersomes.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# вычисление порога методом Оцу и сегментация изображения\n",
    "T_otsu, mask = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# вычислим гистограмму\n",
    "hist = cv2.calcHist([image], [0], None, [256], [0, 256])\n",
    "\n",
    "# графики\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(hist)\n",
    "plt.axvline(x=T_otsu, color=\"r\", linestyle=\"--\", label=f\"T_otsu = {T_otsu}\")\n",
    "plt.xlim([0, 256])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(mask, cmap=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4dea60",
   "metadata": {},
   "source": [
    "#### **Mean-shift сегментация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c3d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем BGR изображение\n",
    "image = cv2.imread(DATA_DIR / \"mountains.jpg\", cv2.IMREAD_COLOR_BGR)\n",
    "\n",
    "# OpenCV Mean Shift сегментация\n",
    "mask = cv2.pyrMeanShiftFiltering(\n",
    "    image, \n",
    "    sp=20, # радиус окрестности для пространственных координат (x, y)\n",
    "    sr=40, # радиус окрестности для цветовых координат (R, G, B)\n",
    "    maxLevel=1 # число уровней в пирамиде Гаусса\n",
    ")\n",
    "\n",
    "# графики\n",
    "mediapy.show_images({\n",
    "    \"исходное изображение\": image[:, :, ::-1], # BGR -> RGB \n",
    "    \"mean-shift сегментация\": mask[:, :, ::-1]\n",
    "}, border=True, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186e214",
   "metadata": {},
   "source": [
    "##### **K-means сегментация DINO2 признаков изображения**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340ac2b",
   "metadata": {},
   "source": [
    "##### **DINOv2 Small Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Callable, Optional, Tuple, Union, Dict, Sequence, Any\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from functools import partial\n",
    "from torch.nn.init import trunc_normal_\n",
    "\n",
    "XFORMERS_ENABLED = False\n",
    "XFORMERS_AVAILABLE = False\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        qkv_bias: bool = False,\n",
    "        proj_bias: bool = True,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim, bias=proj_bias)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0] * self.scale, qkv[1], qkv[2]\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SwiGLUFFN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        act_layer: Callable[..., nn.Module] = None,\n",
    "        drop: float = 0.0,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.w12 = nn.Linear(in_features, 2 * hidden_features, bias=bias)\n",
    "        self.w3 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x12 = self.w12(x)\n",
    "        x1, x2 = x12.chunk(2, dim=-1)\n",
    "        hidden = F.silu(x1) * x2\n",
    "        return self.w3(hidden)\n",
    "\n",
    "def make_2tuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        assert len(x) == 2\n",
    "        return x\n",
    "\n",
    "    assert isinstance(x, int)\n",
    "    return (x, x)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: Union[int, Tuple[int, int]] = 224,\n",
    "        patch_size: Union[int, Tuple[int, int]] = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        norm_layer: Optional[Callable] = None,\n",
    "        flatten_embedding: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        image_HW = make_2tuple(img_size)\n",
    "        patch_HW = make_2tuple(patch_size)\n",
    "\n",
    "        patch_grid_size = image_HW[0] // patch_HW[0], image_HW[1] // patch_HW[1]\n",
    "\n",
    "        self.img_size = image_HW\n",
    "        self.patch_size = patch_HW\n",
    "        self.patches_resolution = patch_grid_size\n",
    "        self.num_patches = patch_grid_size[0] * patch_grid_size[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.flatten_embedding = flatten_embedding\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_HW, stride=patch_HW)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        _, _, H, W = x.shape\n",
    "        \n",
    "        x = self.proj(x)  # B C H W\n",
    "        H, W = x.size(2), x.size(3)\n",
    "        x = x.flatten(2).transpose(1, 2)  # B HW C\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if not self.flatten_embedding:\n",
    "            x = x.reshape(-1, H, W, self.embed_dim)  # B H W C\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def flops(self) -> float:\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        \n",
    "        return flops\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        hidden_features: Optional[int] = None,\n",
    "        out_features: Optional[int] = None,\n",
    "        act_layer: Callable[..., nn.Module] = nn.GELU,\n",
    "        drop: float = 0.0,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        init_values: Union[float, Tensor] = 1e-5,\n",
    "        inplace: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n",
    "    \n",
    "\n",
    "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n",
    "    if drop_prob == 0.0 or not training:\n",
    "        return x\n",
    "    \n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    \n",
    "    if keep_prob > 0.0:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    output = x * random_tensor\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = False,\n",
    "        proj_bias: bool = True,\n",
    "        ffn_bias: bool = True,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        init_values=None,\n",
    "        drop_path: float = 0.0,\n",
    "        act_layer: Callable[..., nn.Module] = nn.GELU,\n",
    "        norm_layer: Callable[..., nn.Module] = nn.LayerNorm,\n",
    "        attn_class: Callable[..., nn.Module] = Attention,\n",
    "        ffn_layer: Callable[..., nn.Module] = Mlp,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # print(f\"biases: qkv: {qkv_bias}, proj: {proj_bias}, ffn: {ffn_bias}\")\n",
    "        \n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = attn_class(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            proj_bias=proj_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = ffn_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "            bias=ffn_bias,\n",
    "        )\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "        self.sample_drop_ratio = drop_path\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        def attn_residual_func(x: Tensor) -> Tensor:\n",
    "            return self.ls1(self.attn(self.norm1(x)))\n",
    "\n",
    "        def ffn_residual_func(x: Tensor) -> Tensor:\n",
    "            return self.ls2(self.mlp(self.norm2(x)))\n",
    "\n",
    "        if self.training and self.sample_drop_ratio > 0.1:\n",
    "            # the overhead is compensated only for a drop path rate larger than 0.1\n",
    "            x = drop_add_residual_stochastic_depth(\n",
    "                x,\n",
    "                residual_func=attn_residual_func,\n",
    "                sample_drop_ratio=self.sample_drop_ratio,\n",
    "            )\n",
    "            x = drop_add_residual_stochastic_depth(\n",
    "                x,\n",
    "                residual_func=ffn_residual_func,\n",
    "                sample_drop_ratio=self.sample_drop_ratio,\n",
    "            )\n",
    "        elif self.training and self.sample_drop_ratio > 0.0:\n",
    "            x = x + self.drop_path1(attn_residual_func(x))\n",
    "            x = x + self.drop_path1(ffn_residual_func(x))  # FIXME: drop_path2\n",
    "        else:\n",
    "            x = x + attn_residual_func(x)\n",
    "            x = x + ffn_residual_func(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def drop_add_residual_stochastic_depth(\n",
    "    x: Tensor,\n",
    "    residual_func: Callable[[Tensor], Tensor],\n",
    "    sample_drop_ratio: float = 0.0,\n",
    ") -> Tensor:\n",
    "    # 1) extract subset using permutation\n",
    "    b, n, d = x.shape\n",
    "    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n",
    "    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n",
    "    x_subset = x[brange]\n",
    "\n",
    "    # 2) apply residual_func to get residual\n",
    "    residual = residual_func(x_subset)\n",
    "\n",
    "    x_flat = x.flatten(1)\n",
    "    residual = residual.flatten(1)\n",
    "\n",
    "    residual_scale_factor = b / sample_subset_size\n",
    "\n",
    "    # 3) add the residual\n",
    "    x_plus_residual = torch.index_add(x_flat, 0, brange, residual.to(dtype=x.dtype), alpha=residual_scale_factor)\n",
    "    return x_plus_residual.view_as(x)\n",
    "\n",
    "\n",
    "def get_branges_scales(x, sample_drop_ratio=0.0):\n",
    "    b, n, d = x.shape\n",
    "    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n",
    "    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n",
    "    residual_scale_factor = b / sample_subset_size\n",
    "    return brange, residual_scale_factor\n",
    "\n",
    "attn_bias_cache: Dict[Tuple, Any] = {}\n",
    "\n",
    "def named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n",
    "    if not depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    for child_name, child_module in module.named_children():\n",
    "        child_name = \".\".join((name, child_name)) if name else child_name\n",
    "        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n",
    "    if depth_first and include_root:\n",
    "        fn(module=module, name=name)\n",
    "    return module\n",
    "\n",
    "\n",
    "class BlockChunk(nn.ModuleList):\n",
    "    def forward(self, x):\n",
    "        for b in self:\n",
    "            x = b(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DinoVisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        ffn_bias=True,\n",
    "        proj_bias=True,\n",
    "        drop_path_rate=0.0,\n",
    "        drop_path_uniform=False,\n",
    "        init_values=None,\n",
    "        embed_layer=PatchEmbed,\n",
    "        act_layer=nn.GELU,\n",
    "        block_fn=Block,\n",
    "        ffn_layer=\"mlp\",\n",
    "        block_chunks=1,\n",
    "        num_register_tokens=0,\n",
    "        interpolate_antialias=False,\n",
    "        interpolate_offset=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 1\n",
    "        self.n_blocks = depth\n",
    "        self.num_heads = num_heads\n",
    "        self.patch_size = patch_size\n",
    "        self.num_register_tokens = num_register_tokens\n",
    "        self.interpolate_antialias = interpolate_antialias\n",
    "        self.interpolate_offset = interpolate_offset\n",
    "\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        assert num_register_tokens >= 0\n",
    "        self.register_tokens = nn.Parameter(torch.zeros(1, num_register_tokens, embed_dim)) if num_register_tokens else None\n",
    "\n",
    "        if drop_path_uniform is True:\n",
    "            dpr = [drop_path_rate] * depth\n",
    "        else:\n",
    "            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        if ffn_layer == \"mlp\":\n",
    "            # print(\"using MLP layer as FFN\")\n",
    "            ffn_layer = Mlp\n",
    "        elif ffn_layer == \"swiglufused\" or ffn_layer == \"swiglu\":\n",
    "            print(\"using SwiGLU layer as FFN\")\n",
    "            assert True\n",
    "            # ffn_layer = SwiGLUFFNFused\n",
    "        elif ffn_layer == \"identity\":\n",
    "            print(\"using Identity layer as FFN\")\n",
    "\n",
    "            def f(*args, **kwargs):\n",
    "                return nn.Identity()\n",
    "\n",
    "            ffn_layer = f\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        blocks_list = [\n",
    "            block_fn(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                proj_bias=proj_bias,\n",
    "                ffn_bias=ffn_bias,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                ffn_layer=ffn_layer,\n",
    "                init_values=init_values,\n",
    "            )\n",
    "            for i in range(depth)\n",
    "        ]\n",
    "\n",
    "        if block_chunks > 0:\n",
    "            self.chunked_blocks = True\n",
    "            chunked_blocks = []\n",
    "            chunksize = depth // block_chunks\n",
    "\n",
    "            for i in range(0, depth, chunksize):\n",
    "                # this is to keep the block index consistent if we chunk the block list\n",
    "                chunked_blocks.append([nn.Identity()] * i + blocks_list[i : i + chunksize])\n",
    "            \n",
    "            self.blocks = nn.ModuleList([BlockChunk(p) for p in chunked_blocks])\n",
    "        else:\n",
    "            self.chunked_blocks = False\n",
    "            self.blocks = nn.ModuleList(blocks_list)\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Identity()\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, embed_dim))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.normal_(self.cls_token, std=1e-6)\n",
    "\n",
    "        if self.register_tokens is not None:\n",
    "            nn.init.normal_(self.register_tokens, std=1e-6)\n",
    "\n",
    "        named_apply(init_weights_vit_timm, self)\n",
    "\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        previous_dtype = x.dtype\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        \n",
    "        pos_embed = self.pos_embed.float()\n",
    "        class_pos_embed = pos_embed[:, 0]\n",
    "        patch_pos_embed = pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_size\n",
    "        h0 = h // self.patch_size\n",
    "        M = int(math.sqrt(N))  # Recover the number of patches in each dimension\n",
    "        kwargs = {}\n",
    "\n",
    "        if self.interpolate_offset:\n",
    "            sx = float(w0 + self.interpolate_offset) / M\n",
    "            sy = float(h0 + self.interpolate_offset) / M\n",
    "            kwargs[\"scale_factor\"] = (sx, sy)\n",
    "        else:\n",
    "            # Simply specify an output size instead of a scale factor\n",
    "            kwargs[\"size\"] = (w0, h0)\n",
    "\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, M, M, dim).permute(0, 3, 1, 2),\n",
    "            mode=\"bilinear\", # \"bilinear\" \"nearest\"  \"bicubic\"\n",
    "            antialias=self.interpolate_antialias,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # assert (w0, h0) == patch_pos_embed.shape[-2:]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1).to(previous_dtype)\n",
    "\n",
    "    def prepare_tokens_with_masks(self, x, masks=None):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        if masks is not None:\n",
    "            x = torch.where(masks.unsqueeze(-1), self.mask_token.to(x.dtype).unsqueeze(0), x)\n",
    "\n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        if self.register_tokens is not None:\n",
    "            x = torch.cat(\n",
    "                (\n",
    "                    x[:, :1],\n",
    "                    self.register_tokens.expand(x.shape[0], -1, -1),\n",
    "                    x[:, 1:],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_features_list(self, x_list, masks_list):\n",
    "        x = [self.prepare_tokens_with_masks(x, masks) for x, masks in zip(x_list, masks_list)]\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        all_x = x\n",
    "        output = []\n",
    "        for x, masks in zip(all_x, masks_list):\n",
    "            x_norm = self.norm(x)\n",
    "            output.append(\n",
    "                {\n",
    "                    \"x_norm_clstoken\": x_norm[:, 0],\n",
    "                    \"x_norm_regtokens\": x_norm[:, 1 : self.num_register_tokens + 1],\n",
    "                    \"x_norm_patchtokens\": x_norm[:, self.num_register_tokens + 1 :],\n",
    "                    \"x_prenorm\": x,\n",
    "                    \"masks\": masks,\n",
    "                }\n",
    "            )\n",
    "        return output\n",
    "\n",
    "    def forward_features(self, x, masks=None):\n",
    "        if isinstance(x, list):\n",
    "            return self.forward_features_list(x, masks)\n",
    "\n",
    "        x = self.prepare_tokens_with_masks(x, masks)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x_norm = self.norm(x)\n",
    "        return {\n",
    "            \"x_norm_clstoken\": x_norm[:, 0],\n",
    "            \"x_norm_regtokens\": x_norm[:, 1 : self.num_register_tokens + 1],\n",
    "            \"x_norm_patchtokens\": x_norm[:, self.num_register_tokens + 1 :],\n",
    "            \"x_prenorm\": x,\n",
    "            \"masks\": masks,\n",
    "        }\n",
    "\n",
    "    def _get_intermediate_layers_not_chunked(self, x, n=1):\n",
    "        x = self.prepare_tokens_with_masks(x)\n",
    "        # If n is an int, take the n last blocks. If it's a list, take them\n",
    "        output, total_block_len = [], len(self.blocks)\n",
    "        blocks_to_take = range(total_block_len - n, total_block_len) if isinstance(n, int) else n\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if i in blocks_to_take:\n",
    "                output.append(x)\n",
    "\n",
    "        assert len(output) == len(blocks_to_take), f\"only {len(output)} / {len(blocks_to_take)} blocks found\"\n",
    "        return output\n",
    "\n",
    "    def _get_intermediate_layers_chunked(self, x, n=1):\n",
    "        x = self.prepare_tokens_with_masks(x)\n",
    "        output, i, total_block_len = [], 0, len(self.blocks[-1])\n",
    "        # If n is an int, take the n last blocks. If it's a list, take them\n",
    "        blocks_to_take = range(total_block_len - n, total_block_len) if isinstance(n, int) else n\n",
    "        for block_chunk in self.blocks:\n",
    "            for blk in block_chunk[i:]:  # Passing the nn.Identity()\n",
    "                x = blk(x)\n",
    "                if i in blocks_to_take:\n",
    "                    output.append(x)\n",
    "                i += 1\n",
    "        assert len(output) == len(blocks_to_take), f\"only {len(output)} / {len(blocks_to_take)} blocks found\"\n",
    "        return output\n",
    "\n",
    "    def get_intermediate_layers(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        n: Union[int, Sequence] = 1,  # Layers or n last layers to take\n",
    "        reshape: bool = False,\n",
    "        return_class_token: bool = False,\n",
    "        norm=True,\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:\n",
    "        if self.chunked_blocks:\n",
    "            outputs = self._get_intermediate_layers_chunked(x, n)\n",
    "        else:\n",
    "            outputs = self._get_intermediate_layers_not_chunked(x, n)\n",
    "\n",
    "        if norm:\n",
    "            outputs = [self.norm(out) for out in outputs]\n",
    "        \n",
    "        class_tokens = [out[:, 0] for out in outputs]\n",
    "        outputs = [out[:, 1 + self.num_register_tokens :] for out in outputs]\n",
    "        \n",
    "        if reshape:\n",
    "            B, _, w, h = x.shape\n",
    "            outputs = [\n",
    "                out.reshape(B, w // self.patch_size, h // self.patch_size, -1).permute(0, 3, 1, 2).contiguous()\n",
    "                for out in outputs\n",
    "            ]\n",
    "        \n",
    "        if return_class_token:\n",
    "            return tuple(zip(outputs, class_tokens))\n",
    "        \n",
    "        return tuple(outputs)\n",
    "\n",
    "    def forward(self, *args, is_training=False, **kwargs):\n",
    "        ret = self.forward_features(*args, **kwargs)\n",
    "        if is_training:\n",
    "            return ret\n",
    "        else:\n",
    "            return self.head(ret[\"x_norm_clstoken\"])\n",
    "\n",
    "\n",
    "def init_weights_vit_timm(module: nn.Module, name: str = \"\"):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        trunc_normal_(module.weight, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "def vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=Attention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_base(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=Attention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_large(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=1024,\n",
    "        depth=24,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=Attention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def vit_giant2(patch_size=16, num_register_tokens=0, **kwargs):\n",
    "    model = DinoVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        embed_dim=1536,\n",
    "        depth=40,\n",
    "        num_heads=24,\n",
    "        mlp_ratio=4,\n",
    "        block_fn=partial(Block, attn_class=Attention),\n",
    "        num_register_tokens=num_register_tokens,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "class ModelWithIntermediateLayers(nn.Module):\n",
    "    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n",
    "        super().__init__()\n",
    "        self.feature_model = feature_model\n",
    "        self.feature_model.eval()\n",
    "        self.n_last_blocks = n_last_blocks\n",
    "        self.autocast_ctx = autocast_ctx\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.inference_mode():\n",
    "            with self.autocast_ctx():\n",
    "                features = self.feature_model.get_intermediate_layers(\n",
    "                    images, self.n_last_blocks, return_class_token=True\n",
    "                )\n",
    "        return features\n",
    "    \n",
    "\n",
    "class DinoBase(nn.Module):\n",
    "    def __init__(self, pretrain=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = DinoVisionTransformer(\n",
    "            patch_size=14,\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            num_heads=12,\n",
    "            mlp_ratio=4,\n",
    "            block_fn=partial(Block, attn_class=Attention),\n",
    "            num_register_tokens=0,\n",
    "            img_size=518,\n",
    "            init_values=1.0,\n",
    "            block_chunks=0\n",
    "        )\n",
    "\n",
    "        mean = torch.as_tensor((0.485, 0.456, 0.406)).view(-1, 1, 1)\n",
    "        std = torch.as_tensor((0.229, 0.224, 0.225)).view(-1, 1, 1)\n",
    "\n",
    "        self.mean = nn.Parameter(mean, requires_grad=False)\n",
    "        self.std = nn.Parameter(std, requires_grad=False)\n",
    "\n",
    "        if pretrain is not None:\n",
    "            state_dict = torch.load(pretrain, map_location=\"cpu\")\n",
    "            state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "            state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "        \n",
    "            self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        return 768\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x.div_(255).sub_(self.mean).div_(self.std)\n",
    "        \n",
    "        # prepare tokens\n",
    "        w, h = x.shape[2:]\n",
    "        x = self.model.patch_embed(x)\n",
    "        x = torch.cat((self.model.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + self.model.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        # forward features\n",
    "        for blk in self.model.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.model.norm(x)\n",
    "        class_tokens = x[:, 0]\n",
    "\n",
    "        return class_tokens\n",
    "    \n",
    "\n",
    "class DinoSmall(nn.Module):\n",
    "    def __init__(self, pretrain=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = DinoVisionTransformer(\n",
    "            patch_size=14,\n",
    "            embed_dim=384,\n",
    "            depth=12,\n",
    "            num_heads=6,\n",
    "            mlp_ratio=4,\n",
    "            block_fn=partial(Block, attn_class=Attention),\n",
    "            num_register_tokens=0,\n",
    "            img_size=518,\n",
    "            init_values=1e-05,\n",
    "            block_chunks=0\n",
    "        )\n",
    "\n",
    "        mean = torch.as_tensor((0.485, 0.456, 0.406)).view(-1, 1, 1)\n",
    "        std = torch.as_tensor((0.229, 0.224, 0.225)).view(-1, 1, 1)\n",
    "\n",
    "        self.mean = nn.Parameter(mean, requires_grad=False)\n",
    "        self.std = nn.Parameter(std, requires_grad=False)\n",
    "\n",
    "        if pretrain is not None:\n",
    "            state_dict = torch.load(pretrain, map_location=\"cpu\")\n",
    "            state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "            state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "            state_dict = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n",
    "        \n",
    "            self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    @property\n",
    "    def feature_dim(self):\n",
    "        return 384\n",
    "    \n",
    "    def forward(self, image):\n",
    "        x = torch.from_numpy(image).float() / 255\n",
    "        x = x.permute(2, 0, 1)\n",
    "        x = x[None]\n",
    "\n",
    "        x.div_(255).sub_(self.mean).div_(self.std)\n",
    "\n",
    "        # prepare tokens\n",
    "        w, h = x.shape[2:]\n",
    "        x = self.model.patch_embed(x)\n",
    "        x = torch.cat((self.model.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = x + self.model.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        # forward features\n",
    "        for blk in self.model.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.model.norm(x)\n",
    "        features = x[:, 1:]\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        features = features[0].reshape(h // 14, w // 14, 384)\n",
    "\n",
    "        return features.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9ffe3",
   "metadata": {},
   "source": [
    "##### **Создадим модель и загрузим предобученные веса**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1276c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DinoSmall(pretrain = DATA_DIR / \"dinov2_vits14_pretrain.pth\")\n",
    "\n",
    "num_params = sum([p.numel() for p in net.parameters() if p.requires_grad])\n",
    "print(f\"число параметров модели: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем RGB изображение\n",
    "image = cv2.imread(DATA_DIR / \"mountains.jpg\", cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "# инференс модели (без вычисления градиентов)\n",
    "with torch.inference_mode():\n",
    "    features = net(image)\n",
    "\n",
    "H, W = image.shape[:2]\n",
    "h, w = H // 14, W // 14\n",
    "\n",
    "# размеры features h x w x 384\n",
    "print(f\"features.shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e42a5",
   "metadata": {},
   "source": [
    "##### **K-means кластеризация**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5931362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# зададим число классов\n",
    "K = 20\n",
    "\n",
    "# k-means кластеризация\n",
    "X = features.reshape(-1, 384) # shape: (h * w) x 384\n",
    "\n",
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=\"auto\") \n",
    "kmeans.fit(X)\n",
    "\n",
    "# ресайз маски в исходный размер изображения\n",
    "mask = kmeans.labels_.reshape(h, w)\n",
    "mask = cv2.resize(mask, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# вычислим выходное сегментационное изображение\n",
    "segmented_image = np.zeros_like(image)\n",
    "\n",
    "for label in range(K):\n",
    "    # координаты кластера\n",
    "    y, x = np.where(mask == label)\n",
    "    # пиксели кластера\n",
    "    cluster = image[y, x]\n",
    "    # средний цвет кластера\n",
    "    cluster_color = np.uint8(cluster.mean(axis=0))\n",
    "    # запишем средний цвет в выходное изображение\n",
    "    segmented_image[y, x] = cluster_color\n",
    "\n",
    "# графики\n",
    "mediapy.show_images({\n",
    "    \"исходное изображение\": image,\n",
    "    \"k-means сегментация\": segmented_image\n",
    "}, border=True, height=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
